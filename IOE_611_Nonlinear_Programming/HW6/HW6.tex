\documentclass[11pt]{article}
\usepackage{geometry}                
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in} 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amsmath, amsfonts, amsthm, amssymb} 
\usepackage[shortlabels]{enumitem}
\usepackage{xcolor}
\usepackage{mathtools}

\parskip = 0.1in

\pagestyle{myheadings}
\markright{Homework of IOE 611 Nonlinear Programming\hfill Yulun Zhuang \hfill}

\input{../611defs}

\newcommand{\oh}{\frac12}
\newcommand{\st}{\text{subject to}}
\newcommand{\gfb}{\nabla f(\bar x)}
\newcommand{\hfb}{H(\bar x)}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}%[section]
\newtheorem{definition}[theorem]{Definition}%[section]
\newtheorem{proposition}[theorem]{Proposition}%[section]
\newtheorem{lemma}[theorem]{Lemma}%[section]
\newtheorem{corollary}[theorem]{Corollary}%[section]
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{exam}{Example}
\newenvironment{solution}
  {\renewcommand\qedsymbol{$\square$}\begin{proof}[\textbf{Solution}]}
  {\end{proof}}
\renewcommand{\proofname}{\textbf{Proof}}

% Colors
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}

% Handy math notations
\newcommand{\grad}{\nabla}
\newcommand{\hess}{\nabla^2}
\newcommand{\tr}{\text{tr}}
\newcommand{\pd}[2][]{ \frac{\partial #1}{\partial #2}} % Partial derivatives
\renewcommand{\d}{{\rm d}}
\newcommand{\ddt}{\frac{\d}{\d t}}
\newcommand{\half}{\frac{1}{2}} % 1/2
\newcommand{\inv}{^{-1}}        % inverse
\newcommand{\T}{^\top}          % transpose

\begin{document}
\title{IOE 611: Homework 6}
\author{Yulun Zhuang}
\maketitle
%**********************************
\section*{Problem 1}
% Exercise 9.12
\textit{Trust region Newton method}.
If $\hess f(x)$ is singular (or very ill-conditioned), the Newton step $\Delta x_{\text{nt}} = -\hess f(x)\inv \grad f(x)$ is not well defined. Instead we can define a search direction $\Delta x_{\text{tr}}$ as the solution of
\begin{align*}
    \text{minimize}\quad & \half v\T H v + g\T v\\
    \text{subject to}\quad & \|v\|_2 \leq \gamma,
\end{align*}
where $H = \hess f(x), g = \grad f(x)$, and $\gamma$ is a positive constant. The point $x + \Delta x_{\text{tr}}$ minimizes the second-order approximation of $f$ at $x$, subject to the constraint that $\| (x + \Delta x_{\text{tr}}) - x\|_2 \leq \gamma$. The set $\{v \mid \|v\|_2\leq \gamma\}$ is called the \textit{trust region}. The parameter $\gamma$, the size of the trust region, reflects our confidence in the second-order model.

Show that $\Delta x_{\text{tr}}$ minimizes
\[
    \half v\T H v + g\T v + \hat \beta \|v\|_2^2,
\]
for some $\hat \beta$. This quadratic function can be interpreted as a regularized quadratic model for $f$ around $x$.

\begin{proof}
    The Lagrangian of the original optimization is
    \[
        \mathcal{L}(v, \lambda) = \half v\T H v + g\T v + \lambda (\|v\|_2 - \gamma)
    \]

    The KKT conditions are
    \begin{align}
        & \lambda \geq \zeros\label{eq:dual-ineq}\\
        & \|v\|_2 \leq \gamma\label{eq:primal-ineq}\\
        & \lambda (\|v\|_2 - \gamma) = \zeros\label{eq:comp-slack}\\
        & \grad \mathcal{L} = (H + 2\lambda I) v - g = \zeros\label{eq:lag-grad}
    \end{align}

    Case 1: $H \succ 0$.
    
    Given $H+2 \lambda I>0$ for all $\lambda \succ 0$, we can solve for $v^* = (H + 2\lambda I)\inv g$.

    Note that $\left\|v^*\right\|_2$ is decreasing with respect to $\lambda$, we know that $\min _{\lambda \geq 0}\left\|v^*\right\|_2=\left\|H^{-1} g\right\|_2$.
    
    If $\left\|H^{-1} g\right\|_2 > \gamma$, we must have $\lambda^*>0$ to satisfy \eqref{eq:primal-ineq}, therefore by \eqref{eq:comp-slack} we have $\left\|\left(H+2 \lambda^* I\right)^{-1} g\right\|_2=\gamma$. 
    
    Let $\hat{\beta}=\lambda^*$, we know that $v^*$ minimizes $\frac{1}{2} v^{\top} H v+g^{\top} v+\hat{\beta}\|v\|^2$ since
    $$
    \frac{\partial}{\partial v}\left(\frac{1}{2} v^{\top} H v+g^{\top} v+\hat{\beta}\|v\|^2\right)=(H+2 \beta I) v+g=\left(H+2 \lambda^* I\right) v+g .
    $$


    If $\left\|H^{-1} g\right\| \leqslant \gamma$, then \eqref{eq:comp-slack} will be satisfied if and only if $\lambda=0$. Similarly we know that $v^*$ minimizes $\frac{1}{2} v^{\top} H v+g^{\top} v$.

    Case 2: $H$ is singular.

    If $\lambda^*>0$, then $\left(H+2 \lambda^* I\right) \succ 0$, and $\gamma=\left\|\left(H+2 \lambda^* I\right)^{-1} g\right\|_2$.

    Let $\hat{\beta}$ be the solution of the above equation, then $v^*$ minimizes $\frac{1}{2} v^{\top} H v+g^{\top} v+\hat{\beta}\|v\|^2$.

    If $\lambda^*=0$, then by \eqref{eq:lag-grad} we have $H v^*=g$, i.e. $g \in \operatorname{Range}(H), v^*=H^{\dagger} g$ and by \eqref{eq:primal-ineq} we have $r \geq \| v^*\|_2=\| H^{\dagger} g \|_2$.

    Therefore, $v^*$ minimizes $\frac{1}{2} v^{\top} H v+g^{\top} v$.


\end{proof}


\clearpage
\section*{Problem 2}
% Exercise 10.15
\textit{Equality constrained entropy maximization}. Consider the equality constrained entropy maximization problem
\begin{align*}
    \text{minimize}\quad & f(x) = \sum_{n}^{i=1} x_i \log x_i\\
    \text{subject to}\quad & Ax = b,
\end{align*}
with $\dom f = \real^n_{++}$ and $A\in\real^{p\times n}$, with $p < n$.

Generate a problem instance with $n = 100$ and $p = 30$ by choosing $A$ randomly (checking that it has full rank), choosing $\hat x$ as a random positive vector (e.g., with entries uniformly distributed on [0, 1]) and then setting $b = A\hat x$. (Thus, $\hat x$ is feasible.)

Compute the solution of the problem using the following methods.

(a) \textit{Standard Newton method}. You can use initial point $x^{(0)} = \hat x$.

(b) \textit{Infeasible start Newton method}. You can use initial point $x^{(0)} = \hat x$ (to compare with the standard Newton method), and also the initial point $x^{(0)} = \ones$.

(c) \textit{Dual Newton method}, i.e. the standard Newton method applied to the dual problem.

Verify that the three methods compute the same optimal point (and Lagrange multiplier).
Compare the computational effort per step for the three methods, assuming relevant
structure is exploited. 
(Your implementation, however, does not need to exploit structure to compute the Newton step.)

\begin{solution}
    \begin{figure}[htb]
        \includegraphics[width=0.6\columnwidth]{"hw6p2.png"}
        \centering
        \caption{Comparisons of three Newton's methods}
        \label{fig:p2}
    \end{figure}


    The computational effort is identical across all three methods. In both standard and infeasible start Newton methods, we solve equations with the coefficient matrix 
    \[
        \begin{bmatrix}
            \hess f(x) & A^\top \\
            A & \zeros
        \end{bmatrix},
    \]
    where $\hess f(x) = \diag(x)^{-1}$.    
    Using block elimination, these equations reduce to a system with the coefficient matrix $A \diag(x) A^\top$.
    
    In the dual method, the equations involve a coefficient matrix of the form $-\hess g(\lambda) = A D A^\top$, where $D = \diag(\exp(-A^\top \lambda - 1))$.
    
    Thus, in all three methods, the primary computational task in each iteration is solving a linear system of the form $A^\top D A z = -g$, where $D$ is a diagonal matrix with positive diagonal elements.
    
\end{solution}




\clearpage
\section*{Problem 3}
% Exercise 11.7
\textit{Tangent to central path}. This problem concerns $dx^*(t)/dt$, which gives the tangent to the central path at the point $x^*(t)$. For simplicity, we consider a problem without equality constraints; the results readily generalize to problems with equality constraints.

(a) Find an explicit expression for $dx^*(t)/dt$.

(b) Show that $f_0(x^*(t))$ decreases as $t$ increases. Thus the objective value in the barrier method decreases, as the parameter $t$ is increased. (We already know that the duality gap, which is $m/t$ decreases as t increases.)

\begin{solution}
    (a) Given the centrality equation
    $$
    0=t \nabla f_0\left(x^*\right)+\sum_{i=1}^m \frac{1}{-f_i\left(x^*\right)} \nabla f_i\left(x^*\right)+A^{\top} \hat{v},
    $$
    
    Differentiate it with respect to $t$, we have    
    $$
    \begin{aligned}
    0 = &\nabla f_0\left(x^*\right) + t \nabla^2 f_0\left(x^*\right) \frac{d x^*}{d t} \\
    &+ \sum_{i=1}^m\left[\frac{1}{f_i^2\left(x^*\right)} \nabla f_i\left(x^*\right) \nabla f_i\left(x^*\right)^{\top} \frac{d x^*}{d t}\right. \\
    & \left.-\frac{1}{f_i\left(x^*\right)} \nabla^2 f_i\left(x^*\right) \frac{d x^*}{d t}\right],
    \end{aligned}
    $$
    
    which gives the explicit expression    
    $$
    \frac{d x^*}{d t}=-\left[t \nabla^2 f_0\left(x^*\right)-\sum_{i=1}^m\left(\frac{1}{f_i\left(x^*\right)} \nabla^2 f_i\left(x^*\right)-\frac{1}{f_i^2\left(x^*\right)} \nabla f_i\left(x^*\right) \nabla f_i\left(x^*\right)^{\top}\right)\right]^{-1} \nabla f_0\left(x^*\right) .
    $$
    
    (b) To show that $f_0(x^*)$ is monotonically decreasing, we can show that $\frac{d}{d t} f_0\left(x^*\right) < 0$.
    
    From (a), we know that $ \frac{d x^*}{d t} = - \mathcal{G}\inv \grad f_0(x^*)$, where $\mathcal{G}\succ 0$.
    
    Therefore    
    $$
    \begin{aligned}
    \frac{d}{d t} f_0\left(x^*\right) & =\nabla f_0\left(x^*\right)^{\top} \frac{d x^*}{d t} \\
    & =-\nabla f_0\left(x^*\right)^{\top} \mathcal{G}^{-1} \nabla f_0\left(x^*\right) \\
    & < 0 .
    \end{aligned}
    $$
\end{solution}

\end{document}
