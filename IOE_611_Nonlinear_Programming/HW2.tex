\documentclass[11pt]{article}
\usepackage{geometry}                
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in} 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amsmath, amsfonts, amsthm, amssymb} 
\usepackage[shortlabels]{enumitem}

\parskip = 0.1in

\pagestyle{myheadings}
\markright{Homework of IOE 611 Nonlinear Programming\hfill Yulun Zhuang \hfill}

\input{611defs}

\newcommand{\oh}{\frac12}
\newcommand{\st}{\text{subject to}}
\newcommand{\gfb}{\nabla f(\bar x)}
\newcommand{\hfb}{H(\bar x)}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}%[section]
\newtheorem{definition}[theorem]{Definition}%[section]
\newtheorem{proposition}[theorem]{Proposition}%[section]
\newtheorem{lemma}[theorem]{Lemma}%[section]
\newtheorem{corollary}[theorem]{Corollary}%[section]
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{exam}{Example}
\newenvironment{solution}
  {\renewcommand\qedsymbol{$\square$}\begin{proof}[Solution]}
  {\end{proof}}


\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\grad}{\nabla}
\newcommand{\hess}{\nabla^2}
\newcommand{\tr}{\text{tr}}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\bS}{\mathbb{S}}

\newcommand{\pd}[2][]{ \frac{\partial #1}{\partial #2}} % Partial derivatives
\renewcommand{\d}{{\rm d}}
\newcommand{\ddt}{\frac{\d}{\d t}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\inv}{^{-1}}
\newcommand{\T}{^\top}

\begin{document}
\title{IOE 611: Homework 2}
\author{Yulun Zhuang}
\maketitle
%**********************************
\section*{Problem 1}
\textit{Inverse of an increasing convex function}. Suppose $f : \RR \rightarrow \RR$ is increasing and convex on its domain $(a, b)$. Let $g$ denote its inverse, i.e., the function with domain $(f(a), f(b))$ and $g(f(x)) = x$ for $a < x < b$. What can you say about convexity or concavity of $g$?

\begin{proof}
  Since $g$ and $f$ are both differentiable, by the chain rule we have
  \begin{align*}
    &g(f(x)) = x\\
    &g'(f(x))f'(x) = 1\\
    \Rightarrow~&g'(f(x)) = \frac{1}{f'(x)} > 0\\
    &g''(f(x))f'(x)^2 + g'(f(x))f''(x) = 0\\
    &g''(f(x))f'(x)^2 + \frac{f''(x)}{f'(x)} = 0\\
    \Rightarrow~& g''(f(x)) = - \frac{f''(x)}{f'(x)^3} < 0
  \end{align*}

  Since $f(x)$ is increasing, $f'(x)>0$, such that $g'>0$. Also because $f(x)$ is convex, $f''(x)>=0, \forall x\in \dom f$, such that $g''<=0$. 
  
  Thus, $g$ is concave.
\end{proof}


\clearpage
\section*{Problem 2}
\textit{A family of concave utility functions}. For $0 < \alpha \leq 1$ let 
\[
u_\alpha(x) = \frac{x^\alpha - 1}{\alpha}
\]
with $\dom u_\alpha = \RR_+$. We also define $u_0(x) = \log x$ (with $\dom u_0 = \RR_{++}$).

(a) Show that for $x>0$, $u_0(x) = \lim_{\alpha \rightarrow 0} u_\alpha(x)$.
\begin{proof}
  Since at $\lim_{\alpha\rightarrow0} u_\alpha(x)$, both numerator and denominator go to zero, by the L'Hopital's rule, we have
  \begin{align*}
    \lim_{\alpha\rightarrow0} u_\alpha(x) 
    &=
    \lim_{\alpha\rightarrow0} \frac{\frac{\dd}{\dd\alpha}(x^\alpha - 1)}{\frac{\dd}{\dd\alpha}\alpha}\\
    &=
    \lim_{\alpha\rightarrow0} \frac{x^\alpha\log(x)}{1}\\
    &=
    \log(x)
  \end{align*}
\end{proof}

(b) Show that $u_\alpha$ are concave, monotone increasing, and all satisfy $u_\alpha(1) = 0$.

\begin{proof}
  For $\alpha\in(0, 1]$
  \begin{align*}
    &u_\alpha(1) = \frac{1^\alpha - 1}{\alpha} = 0\\
    &u_\alpha'(x) = x^{\alpha-1}\geq0 \Rightarrow\text{monotone increasing}\\
    &u_\alpha''(x) = (\alpha-1)x^{\alpha-2} \leq 0 \Rightarrow\text{concave}
  \end{align*}
\end{proof}

\clearpage
\section*{Problem 3}
For each of the following functions determine whether it is convex, concave, quasiconvex, or quasiconcave.

(a) $f(x)=e^x-1$ on $\RR$
\begin{solution}
  \begin{align*}
    f'(x) &= e^x > 0 \Rightarrow \text{monotonic} \Rightarrow \text{quasilinear}\\
    f''(x) &= e^x > 0 \Rightarrow \text{convex, not concave}
  \end{align*}
\end{solution}

(b) $f\left(x_1, x_2\right)=x_1 x_2$ on $\RR_{++}^2$.
\begin{solution}
  \begin{align*}
    \grad f &= 
    \begin{bmatrix}
      x_2 \\x_1
    \end{bmatrix}\\
    \hess f &= 
    \begin{bmatrix}
      0 & 1\\1 & 0
    \end{bmatrix} \\
    &\Rightarrow \text{indefinite} \Rightarrow \text{not convex or concave}
  \end{align*}
  The superlevel sets $\{x_1, x_2\mid x_1x_2\geq \alpha\}$ on $\RR^2_{++}$ are convex, so $f$ is quasiconcave but not quasiconvex.
\end{solution}

(c) $f\left(x_1, x_2\right)=1 /\left(x_1 x_2\right)$ on $\RR_{++}^2$.
\begin{solution}
  \begin{align*}
    \grad f &= 
    \begin{bmatrix}
      -\frac{1}{x_1^2x_2} \\ -\frac{1}{x_1x_2^2}
    \end{bmatrix}\\
    \hess f &= 
    \begin{bmatrix}
      \frac{2}{x_1^3x_2} & \frac{1}{x_1^2x_2^2} \\ \frac{1}{x_1^2x_2^2} & \frac{2}{x_1x_2^3}
    \end{bmatrix}\succ 0 \\
    &\Rightarrow \text{convex, not concave; quasiconvex, not quasiconcave}
  \end{align*}
\end{solution}

(d) $f\left(x_1, x_2\right)=x_1 / x_2$ on $\RR_{++}^2$.
\begin{solution}
  \begin{align*}
    \grad f &= 
    \begin{bmatrix}
      \frac{1}{x_2} \\ -\frac{x_1}{x_2^2}
    \end{bmatrix}\\
    \hess f &= 
    \begin{bmatrix}
      0 & -\frac{1}{x_2^2} \\ -\frac{1}{x_2^2} & \frac{2x_1}{x_2^3}
    \end{bmatrix} \\
    &\Rightarrow \text{indefinite} \Rightarrow \text{not convex or concave}
  \end{align*}
  The sublevel sets of both $f$ and $-f$ are convex, so it is quasilinear.
\end{solution}

(e) $f\left(x_1, x_2\right)=x_1^2 / x_2$ on $\RR \times \RR_{++}$.
\begin{solution}
  \begin{align*}
    \grad f &= 
    \begin{bmatrix}
      \frac{2x_1}{x_2} \\ -\frac{x_1^2}{x_2^2}
    \end{bmatrix}\\
    \hess f &= 
    \begin{bmatrix}
      \frac{2}{x_2} & -\frac{2x_1}{x_2^2} \\ -\frac{2x_1}{x_2^2} & \frac{2x_1^2}{x_2^3}
    \end{bmatrix}
  \end{align*}
  The leading principal minors of $\hess f$ are one positive and one zero, so $\hess f$ is positive semidefinite and $f$ is convex and quasiconvex, but not concave and quasiconcave.
\end{solution}
  
(f) $f\left(x_1, x_2\right)=x_1^\alpha x_2^{1-\alpha}$, where $0 \leq \alpha \leq 1$, on $\RR_{++}^2$.
\begin{solution}
  \begin{align*}
    \grad f &= 
    \begin{bmatrix}
      \alpha x_1^{\alpha-1} x_2^{1-\alpha} \\ (1-\alpha) x_1^\alpha x_2^{-\alpha}
    \end{bmatrix}\\
    \hess f &= 
    \begin{bmatrix}
      \alpha(\alpha-1) x_1^{\alpha-2} x_2^{1-\alpha} & \alpha(1-\alpha) x_1^{\alpha-1} x_2^{-\alpha}\\
      \alpha(1-\alpha) x_1^{\alpha-1} x_2^{-\alpha} & -\alpha(1-\alpha) x_1^{\alpha} x_2^{-\alpha - 1}
    \end{bmatrix}\\
    &= \alpha(1-\alpha)x^{\alpha}x^{1-\alpha}
    \begin{bmatrix}
      -\frac{1}{x_1^2} & \frac{1}{x_1x_2}\\ \frac{1}{x_1x_2} & -\frac{1}{x_2^2}
    \end{bmatrix}\\
    &= -\alpha(1-\alpha)x^{\alpha}x^{1-\alpha}
    \begin{bmatrix}
      \frac{1}{x_1}\\ -\frac{1}{x_2}
    \end{bmatrix}
    \begin{bmatrix}
      \frac{1}{x_1}\\ -\frac{1}{x_2}
    \end{bmatrix}\T \preceq 0\\
    &\Rightarrow \text{concave, not convex; quasiconcave, not quasiconvex}
  \end{align*}
\end{solution}


\clearpage
\section*{Problem 4}
Adapt the proof of concavity of the log-determinant function to show the following.

(a) $f(X) = \tr(X^{-1})$ is convex on $\dom f = \bS^n_{++}$.
\begin{proof}
  Define $g(t) = f(Z+tV)$ where $Z\succ 0$ and $V\in\bS^n$.
  \begin{align*}
    g(t) 
    &= f(Z+tV)\\
    &= \tr\left((Z+tV)^{-1}\right)\\
    &= \tr\left(Z^{-1}\left(I+tZ^{-\half}VZ^{-\half}\right)^{-1}\right)\\
    &= \sum_{i=1}^{n} Z^{-1}_{ii} (1 + t\lambda_i)^{-1}
  \end{align*}
  where $\lambda_i, i=1, \dots, n$ are eigenvalues of $Z^{-\half} V Z^{-\half}$. 
  
  We have $Z^{-1}_{ii} > 0$ because the inverse of a positive definite matrix is also positive definite. $g(t)$ is convex since it is a positive weighted sum of convex functions $(1 + t\lambda_i)^{-1}$. Thus, $f(X)$ is convex. 
\end{proof}

(b) $f(X) = (\det X)^{1/n}$ is concave on $\dom f = \bS^n_{++}$.
\begin{proof}
  Define $g(t) = f(Z+tV)$ where $Z\succ 0$ and $V\in\bS^n$.
  \begin{align*}
    g(t) & =(\det(Z+t V))^{1 / n} \\
    & =\left(\det Z^{1 / 2} \det\left(I+t Z^{-1 / 2} V Z^{-1 / 2}\right) \det Z^{1 / 2}\right)^{1 / n} \\
    & =(\det Z)^{1 / n}\left(\prod_{i=1}^n\left(1+t \lambda_i\right)\right)^{1 / n}
  \end{align*}
  where $\lambda_i, i=1, \dots, n$ are eigenvalues of $Z^{-\half} V Z^{-\half}$.

  $g(t)$ is concave since $\det Z > 0$ and the geometric mean $\left(\prod_{i=1}^{n}x_i\right)^{1/n}$ is concave on $\RR^n_{++}$. Thus $f(X)$ is concave.
\end{proof}


\clearpage
\section*{Problem 5}
\textit{Perspective of a function}.

(a) Show that for $p>1$,
\[
f(x, t) = \frac{|x_1|^p + \dots + |x_n|^p}{t^{p-1}} = \frac{\|x\|^p_p}{t^{p-1}}
\]
is convex on $\{(x, t)\mid t>0\}$

\begin{proof}
  Define $g(x) = \|x\|^p_p, \dom g=\{x\mid x\in \RR^n\}$ which is convex. The perspective function of $g(x)$ on $\{(x, t)\mid x/t\in\dom g, t>0\}$ is
  \begin{align*}
    tg(x/t) &= t\frac{\|x\|^p_p}{t^p} = \frac{\|x\|^p_p}{t^{p-1}} = f(x, t)
  \end{align*}
  
  Thus, $f(x, t), \dom f=\{(x, t)\mid t>0\}$ is the perspective function of a convex function $\|x\|^p_p$, so it is convex.
\end{proof}


(b) Show that
\[
f(x) = \frac{\|Ax+b\|^2_2}{c\T x+d}
\]
is convex on $\{x\mid c\T x+d > 0\}$, where $A\in \RR^{m\times n}, b\in \RR^m, c\in\RR^n$ and $d\in \RR$.

\begin{proof}
  Let $g(x)=\|A x+b\|_2^2, \dom g=\{x\mid x\in \RR^n\}$, then $\nabla^2 g(x)=2 A^{\top} A \succeq 0$, which implies $g$ is convex.
  
  Therefore, the perspective function
  $$
  h(x, t)=\frac{\|A x+\|_2^2}{t}, \quad \dom h=\left\{(x, t) \mid x \in \mathbb{R}^n, t>0\right\}
  $$  
  is also convex.
  Since $f(x)=h\left(x, c^{\top} x+d\right), \dom f=\left\{x \mid c^{\top} x+d>0\right\}$ is a composition of an affine function and a convex function, then it is also a convex function.
\end{proof}

\clearpage
\section*{Problem 6}
\textit{Subgradient}. A vector $g\in \RR^n$ is a subgradient of function $f:\RR^n \rightarrow \RR$ at point $x\in\dom f$ if
\[
f(y) \geq f(x) + g\T (y-x)
\] 
for any $y\in\dom f$.

(a) Suppose $f$ is a convex function, and $x\in\intr\dom f$. Prove that there exists a subgradient of $f$ at $x$.




(b) Suppose $x\in\dom f$, but is not an interior point of the domain. Does there (always) exist a subgradient of $f$ at $x$?


\clearpage
\section*{Problem 7}
Show that the so-called logarithmic barrier for the second order cone, $f(x, t) = -log(t^2 - x\T x)$ with $\dom f = \{(x,t) \in \RR^{n+1} | t>\|x\|_2\}$, is convex.
While this can be done by demonstrating that the Hessian of f is positive semidefinite everywhere on its domain, the following is an outline of a much simpler and more elegant proof:

(a) Show that $t-\frac{u\T u}{t}$ is a concave function on $\dom f$.
\begin{proof}
  Let $g(u) = u\T u, \dom g=\{u\in \RR^n\}$, which is convex, hence $h(u, t) = u\T u/t, \dom h = \{(u, t)\in \RR^{n+1} \mid u/t \in \dom g, t>\|u\|_2\}$ is the perspective of $g(u)$ so it is convex. Note that $- \frac{u\T u}{t}$ is concave and $t-\frac{u\T u}{t}>0$ is a combination of an affine function and a concave function so it is also concave.
\end{proof}

(b) Show that $-\log (t - \frac{u\T u}{t})$ is a convex function on $\dom f$.
\begin{proof}
  Since the negative logarithm is convex, its extended-value extension is non-increasing. By the composition rules, given $h(u, t)$ is concave, $-\log(h(u,t))$ is convex.
\end{proof}

(c) Show that $f$ is convex.
\begin{proof}
  \begin{align*}
    f(x, t)
    &= -\log(t^2 - x\T x)\\
    &= -\log(t\cdot h(x, t))\\
    &= -\log(t) - \log(h(x, t))
  \end{align*}
  which is a sum of two convex function and thus also convex on $\dom f$.
\end{proof}

\clearpage
\section*{Problem 8}
Suppose that $f(x): \RR^n \rightarrow \RR$ is a twice differentiable function (but not necessarily convex). Show that if $\bar x$ is a local minimum of $f(x)$, then we must have $\grad f(\bar x) = 0$.

\begin{proof}
  Given $\bar x$ is a local minimum of $f(x)$, there exists $\epsilon>0, y\in \RR^n, y\neq x$, such that $f(y) \geq f(\bar x)$ for $\|y - \bar x\| \leq \epsilon$.

  Since $f(x)$ is twice differentiable, expand $f(y) = f(\bar x) + \grad f(\bar x) (y - \bar x) \geq f(\bar x)$, i.e. 
  $$\grad f(\bar x) (y - \bar x) \geq 0$$

  Choose $y_1>\bar x$ and $y_2 < \bar x$, the inequality must holds for both $y$, thus 
  \[
    \grad f(\bar x) (y - \bar x) = 0 \Rightarrow \grad f(\bar x) = 0
  \]
\end{proof}


\clearpage
\section*{Problem 9}
Given a set of points $\{v_1, v_2, \dots, v_k\}$, define
\[
\Co\{v_1, v_2, \dots, v_k\} = \{\alpha_1 v_1 + \dots + \alpha_k v_k\mid \alpha_1 + \dots \alpha_k = 1, \alpha_1, \dots, \alpha_k \geq 0\}
\]

Show that the maximum of a convex function $f$ over $\Co\{v_1, v_2, \dots, v_k\}$ is achieved at one of its vertices, i.e.,
\[
\sup_{x\in\Co\{v1, \dots, v_k\}} f(x) = \max_{1\leq i \leq k} f(v_i)
\]

\begin{proof}
  Let $x = \alpha_1 v_1 + \dots + \alpha_k v_k \in \Co\{v_1, v_2, \dots, v_k\}$,
  \begin{align*}
    f(x) 
    &= f(\alpha_1 v_1 + \dots + \alpha_k v_k)\\
    &\leq \alpha_1 f(v_1) + \dots + \alpha_k f(v_k)\\
    &\leq \max_{1\leq i \leq k} f(v_i)\\
    &=\sup_x f(x)
  \end{align*}
  That is, the maximum of $f(x)$ is achieved at one of its vertices $f(v_i)$.
\end{proof}

\end{document}
